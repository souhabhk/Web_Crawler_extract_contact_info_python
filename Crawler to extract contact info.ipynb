{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b07281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests.exceptions\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlparse\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39afdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_contact_info(html):\n",
    " \n",
    " email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b')\n",
    " phone_pattern = re.compile(r'\\+\\d+[-\\.\\s]??\\d+[-\\.\\s]??\\d+[-\\.\\s]??\\d+[-\\.\\s]??\\d+[-\\.\\s]??\\d+')\n",
    " emails = set(re.findall(email_pattern, soup.text))\n",
    " phones = set(re.findall(phone_pattern, soup.text))\n",
    " return emails, phones\n",
    "\n",
    "\n",
    "def get_contact_info(url):\n",
    " response = requests.get(url)\n",
    " if response.status_code == 200:\n",
    "        contacts = extract_contact_info(response.text)\n",
    "        emails = ', '.join(contacts[0]) if contacts[0] else 'N/A'\n",
    "        phones = ', '.join(contacts[1]) if contacts[1] else 'N/A'\n",
    "        # Check if emails and phones lists are empty\n",
    "        if not emails:\n",
    "            emails = 'N/A'\n",
    "        if not phones:\n",
    "            phones = 'N/A'\n",
    " return emails, phones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1524e0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://orange.tn\n",
      "Processing https://orange.tn/contact\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "url = \"https://orange.tn\" #put the targeted company's url here\n",
    "# a queue of urls to be crawled\n",
    "new_urls = deque([url])\n",
    "\n",
    "# a set of urls that we have already been processed \n",
    "processed_urls = set()\n",
    "# a set of domains inside the target website\n",
    "local_urls = set()\n",
    "# a set of domains outside the target website\n",
    "foreign_urls = set()\n",
    "# a set of broken urls\n",
    "broken_urls = set()\n",
    "# process urls one by one until we exhaust the queue\n",
    "while len(new_urls):\n",
    "    # move next url from the queue to the set of processed urls\n",
    "    url = new_urls.popleft()\n",
    "    processed_urls.add(url)\n",
    "    # get url's content\n",
    "    print(\"Processing %s\" % url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, requests.exceptions.InvalidURL, requests.exceptions.InvalidSchema):\n",
    "        # add broken urls to it's own set, then continue\n",
    "        broken_urls.add(url)\n",
    "        continue\n",
    "    \n",
    "    # extract base url to resolve relative links\n",
    "    parts = urlsplit(url)\n",
    "    base = \"{0.netloc}\".format(parts)\n",
    "    strip_base = base.replace(\"www.\", \"\")\n",
    "    base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "    path = url[:url.rfind('/')+1] if '/' in parts.path else url\n",
    "\n",
    "    # create a beutiful soup for the html document\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    for link in soup.find_all('a'):\n",
    "        # extract link url from the anchor\n",
    "        anchor = link.attrs[\"href\"] if \"href\" in link.attrs else ''\n",
    "        if anchor.startswith('/contact'):\n",
    "            local_link = base_url + anchor\n",
    "            local_urls.add(local_link)\n",
    "        for i in local_urls:\n",
    "            if not i in new_urls and not i in processed_urls:\n",
    "                new_urls.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456f903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails: N/A\n",
      "Phones: +21631111150, +21631111151, +216 3001 3001\n"
     ]
    }
   ],
   "source": [
    "for item in processed_urls:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    html = response.text\n",
    "    contact_info = get_contact_info(item)\n",
    "    if contact_info is not None:\n",
    "        emails, phones = contact_info\n",
    "        print('Emails:', emails)\n",
    "        print('Phones:', phones)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40365da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
